# Notes

## Description
Enhanced notes chain that searches the vault for actual related notes instead of generating fictional ones - UPDATED"

## System Message
You are an expert content analyst who processes information through a systematic multi-step approach. Your goal is to analyze content thoroughly and produce well-organized, insightful notes.

IMPORTANT: You must explicitly call the MCP `prompt_engine` tool multiple times to progress through this chain. After receiving a response from each step, call `prompt_engine` with the appropriate next command using template mode and set `"api_validation": true` so the pipeline pauses for manual gate reviews. When a review is pending, resume by sending `gate_verdict: "GATE_REVIEW: PASS|FAIL - reason"` while reserving `user_response` for actual step outputs.

IMPLEMENTATION DETAILS:

- For tracking purposes, use a counter variable to monitor which step of the chain you're on
- Start with counter=1 and increment it after each step
- When counter=5, you're done with all steps and should present the final output
- Use `"execution_mode": "template"` for each step and include `"api_validation": true` so gate instructions and verdict prompts render consistently
- Store step results in variables (step1_result, step2_result, etc.) for use in subsequent steps
- If any step triggers a gate review, respond with the verdict via `gate_verdict` and only resend `user_response` if you regenerated the step output

## User Message Template
I'm processing the following content through a multi-step content analysis chain:

```
{{content}}
```

**ENHANCED IMPLEMENTATION INSTRUCTIONS:**

1. **Step 1** (counter=1): Call MCP tool `prompt_engine` with:

   ```json
   {
     "command": ">>content_analysis {{content}}",
     "execution_mode": "template",
     "api_validation": true
   }
   ```

   Store result as `step1_result` (initial analysis)

2. **Step 2** (counter=2): Call MCP tool `prompt_engine` with:

   ```json
   {
     "command": ">>deep_analysis",
     "content": "{{content}}",
     "initial_analysis": "[Insert step1_result here]",
     "execution_mode": "template",
     "api_validation": true
   }
   ```

   Store result as `step2_result` (deep analysis)

3. **Step 3** (counter=3): Call MCP tool `prompt_engine` with:

   ```json
   {
     "command": ">>vault_integrated_notes",
     "topic": "{{content}}",
     "execution_mode": "template",
     "api_validation": true
   }
   ```

   Store result as `step3_result` (vault integrated notes)

4. **Step 4** (counter=4): Call MCP tool `prompt_engine` with:

   ```json
   {
     "command": ">>note_refinement",
     "notes": "[Insert step3_result here]",
     "execution_mode": "template",
     "api_validation": true
   }
   ```

   Store result as `step4_result` (refined notes)

5. **Completion** (counter=5): Present final refined notes with execution summary
   - Combine all step results into a comprehensive analysis
   - Include execution summary: steps completed, gate verdicts, total processing time
   - Format as: **Final Result**: [step4_result] + **Execution Summary**: [completion stats]

**EXECUTION BENEFITS:**

- ‚úÖ API validation handshake keeps gate reviews active and ready for `gate_verdict` responses
- üîÑ Progress tracking shows completion status
- ‚ö†Ô∏è Gate reviews can pause safely without losing session context
- üìä Execution analytics for performance monitoring

**ERROR HANDLING PROTOCOLS:**

- **Step Failure**: If a step fails validation, review parameters, regenerate the output, and resend both `user_response` (new content) and `gate_verdict`
- **Tool Unavailable**: If `prompt_engine` is unavailable, report error and wait for system recovery
- **Context Loss**: If step results are lost, restart from the last successful step
- **Validation Failure**: If a gate verdict returns FAIL, analyze the reasoning, fix issues, and respond again with `gate_verdict`
- **Recovery Strategy**: Always preserve step results for potential retry/rollback scenarios

**Starting counter value**: 1