// @lifecycle canonical - LLM client wrappers used by semantic analyzer.
/**
 * LLM Integration Clients for Semantic Analysis
 *
 * Provides concrete implementations for different LLM providers
 * to enable intelligent semantic analysis when configured.
 */
/**
 * Base LLM client with common functionality
 */
class BaseLLMClient {
    constructor(logger, config) {
        this.logger = logger;
        this.config = config;
    }
    /**
     * Common prompt construction for semantic analysis
     */
    buildAnalysisPrompt(request) {
        return `${request.task}

Prompt Text:
"""
${request.text}
"""

Available Execution Types: ${request.categories.join(', ')}
Available Methodologies: ${request.methodologies.join(', ')}

Please analyze this prompt and return:
1. executionType: One of [${request.categories.join(', ')}]
2. confidence: Number between 0 and 1
3. reasoning: Array of strings explaining your analysis
4. recommendedFramework: One of [${request.methodologies.join(', ')}] or "none"
5. complexity: One of ["low", "medium", "high"]

Respond in JSON format only.`;
    }
    /**
     * Parse LLM response and validate format
     */
    parseResponse(response) {
        try {
            const parsed = JSON.parse(response);
            return {
                executionType: parsed.executionType || 'single',
                confidence: Math.max(0.1, Math.min(1.0, parsed.confidence || 0.5)),
                reasoning: Array.isArray(parsed.reasoning) ? parsed.reasoning : ['LLM analysis performed'],
                recommendedFramework: parsed.recommendedFramework === 'none' ? undefined : parsed.recommendedFramework,
                complexity: ['low', 'medium', 'high'].includes(parsed.complexity)
                    ? parsed.complexity
                    : 'medium',
            };
        }
        catch (error) {
            this.logger.warn('Failed to parse LLM response, using defaults:', error);
            return {
                executionType: 'template',
                confidence: 0.3,
                reasoning: ['Failed to parse LLM response'],
                complexity: 'medium',
            };
        }
    }
}
/**
 * OpenAI client implementation
 */
export class OpenAIClient extends BaseLLMClient {
    async classify(request) {
        if (!this.config.apiKey) {
            throw new Error('OpenAI API key not configured');
        }
        const prompt = this.buildAnalysisPrompt(request);
        try {
            const response = await fetch(this.config.endpoint || 'https://api.openai.com/v1/chat/completions', {
                method: 'POST',
                headers: {
                    Authorization: `Bearer ${this.config.apiKey}`,
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                    model: this.config.model,
                    messages: [
                        {
                            role: 'system',
                            content: 'You are an expert at analyzing prompts for execution strategy and framework requirements. Always respond with valid JSON.',
                        },
                        {
                            role: 'user',
                            content: prompt,
                        },
                    ],
                    max_tokens: this.config.maxTokens,
                    temperature: this.config.temperature,
                }),
            });
            if (!response.ok) {
                throw new Error(`OpenAI API error: ${response.status} ${response.statusText}`);
            }
            const data = (await response.json());
            const content = data.choices?.[0]?.message?.content;
            if (!content) {
                throw new Error('No content in OpenAI response');
            }
            this.logger.debug('OpenAI analysis completed successfully');
            return this.parseResponse(content);
        }
        catch (error) {
            this.logger.error('OpenAI API call failed:', error);
            throw error;
        }
    }
}
/**
 * Anthropic client implementation
 */
export class AnthropicClient extends BaseLLMClient {
    async classify(request) {
        if (!this.config.apiKey) {
            throw new Error('Anthropic API key not configured');
        }
        const prompt = this.buildAnalysisPrompt(request);
        try {
            const response = await fetch(this.config.endpoint || 'https://api.anthropic.com/v1/messages', {
                method: 'POST',
                headers: {
                    Authorization: `Bearer ${this.config.apiKey}`,
                    'Content-Type': 'application/json',
                    'anthropic-version': '2023-06-01',
                },
                body: JSON.stringify({
                    model: this.config.model || 'claude-3-haiku-20240307',
                    max_tokens: this.config.maxTokens,
                    temperature: this.config.temperature,
                    messages: [
                        {
                            role: 'user',
                            content: prompt,
                        },
                    ],
                }),
            });
            if (!response.ok) {
                throw new Error(`Anthropic API error: ${response.status} ${response.statusText}`);
            }
            const data = (await response.json());
            const content = data.content?.[0]?.text;
            if (!content) {
                throw new Error('No content in Anthropic response');
            }
            this.logger.debug('Anthropic analysis completed successfully');
            return this.parseResponse(content);
        }
        catch (error) {
            this.logger.error('Anthropic API call failed:', error);
            throw error;
        }
    }
}
/**
 * Custom endpoint client implementation
 */
export class CustomClient extends BaseLLMClient {
    async classify(request) {
        if (!this.config.endpoint) {
            throw new Error('Custom endpoint not configured');
        }
        const prompt = this.buildAnalysisPrompt(request);
        try {
            const headers = {
                'Content-Type': 'application/json',
            };
            if (this.config.apiKey) {
                headers['Authorization'] = `Bearer ${this.config.apiKey}`;
            }
            const response = await fetch(this.config.endpoint, {
                method: 'POST',
                headers,
                body: JSON.stringify({
                    model: this.config.model,
                    prompt: prompt,
                    max_tokens: this.config.maxTokens,
                    temperature: this.config.temperature,
                }),
            });
            if (!response.ok) {
                throw new Error(`Custom endpoint error: ${response.status} ${response.statusText}`);
            }
            const data = (await response.json());
            // Assume the custom endpoint returns the analysis directly
            const content = data.response || data.content || data.text;
            if (!content) {
                throw new Error('No content in custom endpoint response');
            }
            this.logger.debug('Custom endpoint analysis completed successfully');
            return this.parseResponse(content);
        }
        catch (error) {
            this.logger.error('Custom endpoint call failed:', error);
            throw error;
        }
    }
}
/**
 * Detect LLM provider from endpoint URL
 */
function detectProviderFromEndpoint(endpoint) {
    if (!endpoint) {
        throw new Error('Endpoint URL is required for provider auto-detection');
    }
    const url = endpoint.toLowerCase();
    if (url.includes('api.openai.com')) {
        return 'openai';
    }
    else if (url.includes('api.anthropic.com')) {
        return 'anthropic';
    }
    else {
        return 'custom';
    }
}
/**
 * LLM client factory
 */
export class LLMClientFactory {
    static create(logger, config) {
        try {
            const provider = detectProviderFromEndpoint(config.endpoint);
            switch (provider) {
                case 'openai':
                    return new OpenAIClient(logger, config);
                case 'anthropic':
                    return new AnthropicClient(logger, config);
                case 'custom':
                    return new CustomClient(logger, config);
                default:
                    throw new Error(`Unsupported LLM provider: ${provider}`);
            }
        }
        catch (error) {
            throw new Error(`Failed to create LLM client: ${error instanceof Error ? error.message : 'Unknown error'}. ` +
                `Please ensure the endpoint URL is valid and follows the format: ` +
                `"https://api.openai.com/v1/chat/completions" for OpenAI, ` +
                `"https://api.anthropic.com/v1/messages" for Anthropic, ` +
                `or a custom endpoint URL for other providers.`);
        }
    }
    /**
     * Test LLM client configuration
     */
    static async testClient(logger, config) {
        try {
            // Auto-detect provider before testing
            const provider = detectProviderFromEndpoint(config.endpoint);
            logger.debug(`Auto-detected LLM provider: ${provider} from endpoint: ${config.endpoint}`);
            const client = LLMClientFactory.create(logger, config);
            const testResult = await client.classify({
                text: 'Analyze this simple test prompt with two arguments: {{input}} and {{context}}',
                task: 'Test classification',
                categories: ['prompt', 'template'],
                methodologies: ['CAGEERF', 'none'],
            });
            // Basic validation that we got a valid response
            return Boolean(testResult.executionType && testResult.confidence > 0 && testResult.reasoning.length > 0);
        }
        catch (error) {
            logger.error('LLM client test failed:', error);
            return false;
        }
    }
}
/**
 * Environment variable configuration helper
 * Provider is auto-detected from endpoint URL
 */
export function loadLLMConfigFromEnv() {
    return {
        enabled: process.env.MCP_LLM_ENABLED === 'true',
        apiKey: process.env.MCP_LLM_API_KEY || null,
        endpoint: process.env.MCP_LLM_ENDPOINT || null,
        model: process.env.MCP_LLM_MODEL || 'gpt-4',
        maxTokens: parseInt(process.env.MCP_LLM_MAX_TOKENS || '1000'),
        temperature: parseFloat(process.env.MCP_LLM_TEMPERATURE || '0.1'),
    };
}
//# sourceMappingURL=llm-clients.js.map